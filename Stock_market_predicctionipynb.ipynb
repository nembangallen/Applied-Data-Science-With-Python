{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Stock market predicctionipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nembangallen/Applied-Data-Science-With-Python/blob/master/Stock_market_predicctionipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAi5kzfARZPc"
      },
      "source": [
        "### Stock Market Prediction And Forecasting Using Stacked LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4HjgLyeRZPf"
      },
      "source": [
        "### Keras and Tensorflow >2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNdhn14zRZPh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXE131yRc6aI"
      },
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, time_step=1):\n",
        "\tdataX, dataY = [], []\n",
        "\tfor i in range(len(dataset)-time_step-1):\n",
        "\t\ta = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
        "\t\tdataX.append(a)\n",
        "\t\tdataY.append(dataset[i + time_step, 0])\n",
        "\treturn np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-7iyj77iswo"
      },
      "source": [
        "def pre_processing(df1):\n",
        "  '''Data Processing modeule\n",
        "  1. take only closing value from dataframe\n",
        "  2. scale value from 0 to 1 taking minimum and maxixum value from closing value\n",
        "  3. split to train and test 70-30 %\n",
        "  4. return train and test data\n",
        "  '''\n",
        "  df1=df.reset_index()['c']   #read only closing value\n",
        "  ### LSTM are sensitive to the scale of the data. so we apply MinMax scaler \n",
        "  scaler=MinMaxScaler(feature_range=(0,1))\n",
        "  df1=scaler.fit_transform(np.array(df1).reshape(-1,1))\n",
        "  ##splitting dataset into train and test split\n",
        "  training_size=int(len(df1)*0.65)\n",
        "  test_size=len(df1)-training_size\n",
        "  train_data,test_data=df1[0:training_size,:],df1[training_size:len(df1),:1]\n",
        "  # reshape into X=t,t+1,t+2,t+3 and Y=t+4\n",
        "  time_step = 100\n",
        "  X_train, y_train = create_dataset(train_data, time_step)\n",
        "  X_test, ytest = create_dataset(test_data, time_step)\n",
        "  # reshape input to be [samples, time steps, features] which is required for LSTM\n",
        "  X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
        "  X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
        "  return X_train,y_train, X_test,ytest,test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHgA8PIAiyeq"
      },
      "source": [
        "# Build LSTM\n",
        "def Build_model():\n",
        "  ''' LSTM Module\n",
        "      1. create sequential model\n",
        "      2. add 3 lstm layer with 50 lstm node\n",
        "      3. add dense layer with 1 node as output\n",
        "      4. return model\n",
        "  '''\n",
        "  model=Sequential()\n",
        "  model.add(LSTM(50,return_sequences=True,input_shape=(100,1)))\n",
        "  model.add(LSTM(50,return_sequences=True))\n",
        "  model.add(LSTM(50))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error',optimizer='adam')\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcBuJ7Y0i95x"
      },
      "source": [
        "# Train model\n",
        "def train_model(model,X_train,y_train,X_test,ytest,epochs):\n",
        "  '''This module is for training the model'''\n",
        "  model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=epochs,batch_size=64,verbose=1)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L68GJxMjD5T"
      },
      "source": [
        "### Lets Do the prediction and check performance metrics\n",
        "def test_model(model,X_train,X_test):\n",
        "  train_predict=model.predict(X_train)\n",
        "  test_predict=model.predict(X_test)\n",
        "  ##Transformback to original form\n",
        "  train_predict=scaler.inverse_transform(train_predict)\n",
        "  test_predict=scaler.inverse_transform(test_predict)\n",
        "  return train_predict, test_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0tHfw8AjMmo"
      },
      "source": [
        "#forcasting\n",
        "def forcase_data(model,test_data,n_steps=100):\n",
        "  x_input=test_data[len(test_data)-100:].reshape(1,-1)\n",
        "  temp_input=list(x_input)\n",
        "  temp_input=temp_input[0].tolist()\n",
        "  # demonstrate prediction for next 10 days\n",
        "  lst_output=[]\n",
        "  i=0\n",
        "  while(i<7):\n",
        "      if(len(temp_input)>100):\n",
        "          #print(temp_input)\n",
        "          x_input=np.array(temp_input[1:])\n",
        "          print(\"{} day input {}\".format(i,x_input))\n",
        "          x_input=x_input.reshape(1,-1)\n",
        "          x_input = x_input.reshape((1, n_steps, 1))\n",
        "          #print(x_input)\n",
        "          yhat = model.predict(x_input, verbose=0)\n",
        "          print(\"{} day output {}\".format(i,yhat))\n",
        "          temp_input.extend(yhat[0].tolist())\n",
        "          temp_input=temp_input[1:]\n",
        "          #print(temp_input)\n",
        "          lst_output.extend(yhat.tolist())\n",
        "          i=i+1\n",
        "      else:\n",
        "          x_input = x_input.reshape((1, n_steps,1))\n",
        "          yhat = model.predict(x_input, verbose=0)\n",
        "          print(yhat[0])\n",
        "          temp_input.extend(yhat[0].tolist())\n",
        "          print(len(temp_input))\n",
        "          lst_output.extend(yhat.tolist())\n",
        "          i=i+1\n",
        "  return lst_output\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwk3mEOborY1"
      },
      "source": [
        "def Save_Model(model,path,model_name):\n",
        "  # serialize model to JSON\n",
        "  save_to_json = model_name+'.json'\n",
        "  save_to_h5 = model_name+'.h5'\n",
        "  model_json = model.to_json()\n",
        "  with open(save_to_json, \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(save_to_h5)\n",
        "  print(\"Saved model {} to disk\".format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4tc7coJXAzM"
      },
      "source": [
        "PATH = os.getcwd()\n",
        "nepsec_data_path = PATH + '/nepsedata' #give SOURCE  data  PATH\n",
        "nepsec_trainned_path = PATH + '/nepsecModels' \n",
        "dataname_list = []\n",
        "data_list = []\n",
        "for x in os.listdir(nepsec_data_path):\n",
        "    if x.endswith(\".csv\"):\n",
        "      dataname_list.append(x)\n",
        "      df=pd.read_csv(nepsec_data_path+'/'+x)\n",
        "      print(\"reading data {} complete\".format(x))\n",
        "      data_list.append(df)\n",
        "      X_train,y_train, X_test,ytest,test_data = pre_processing(df1=df)\n",
        "      model = Build_model()\n",
        "      model = train_model(model=model,X_train=X_train,y_train=y_train,X_test=X_test,ytest=ytest,epochs=1)\n",
        "      model_name = x.split(\".\")\n",
        "      Save_Model(model=model,path=nepsec_trainned_path,model_name=model_name[0])\n",
        "      # train_predict, test_predict = test_model(model=model,X_train=X_train,X_test=X_test)\n",
        "      # lst_output = forcase_data(model=model,test_data=test_data,n_steps=100)\n",
        "      # print(lst_output)\n",
        "    else:\n",
        "      pass\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgwsbMSYZAGg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}